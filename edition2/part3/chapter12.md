![ch12](../img/chapter12.png)

# 第十二章：数据系统展望

> 复杂的系统都是从简单的系统演进而来的，从另一个角度来说，开始就很复杂的系统是根本不会被设计出来的。
>
> — 约翰·加尔, *Systemantics* (1975)

---
早起发布读者须知

作为早期发布电子书，您可获取书籍的最初版本——作者写作时的原始未编辑内容——因此您能够早在这些书籍正式发布之前，就充分利用这些技术内容。

这将是本书终版的第十二章。本书的GitHub代码库位于
[https://github.com/ept/ddia2-feedback](https://github.com/ept/ddia2-feedback).

若您希望积极参与此版草稿的审阅与评论工作，请通过GitHub联系我们。
--- 


在[第十一章](chapter11.md)中我们讨论了批处理-它是将一组文件作为输出然后在产出一组新的文件的技术。输出是**派生数据**（*derived data*）的一种形式；运行在批处理流程中的数据在必要的时候是可以再次生成的。我们了解到了这一简单却强大的理念是如何被应用于构建搜索索引、推荐系统、分析工具等领域。

但是，在[第十一章](chapter11.md)中我们始终假设的一个命题是：输入是有边界的-即已知且有限的大小-所以批处理程序知道它何时完成了输入信息的读取。例如MapReduce的核心排序操作必须完整的读取所有的输入后才能开始输出结果：这就有可能会发生一个很小的键值数据记录很晚才被输入，因为这个小键值记录需要我们第一个输出，所以我们的输出不能很早进行（要等待这个最小键值输入后才能开始输出操作）。

实际上，很多数据之所以无边界是因为它们是随着时间的推移慢慢到达的：你的用户昨天和今天产生了数据，它明天还会继续产生更多的数据。除非你歇业，否者这个过程永远不会停止，所以在一定意义上，数据集从来不会有“完成（complete）”态\[[1](#ch12References1)]。因此，批处理程序必须人工的将数据划分为固定区间的数据块：例如：在每天结束的时候处理一天的有效数据，或者在每个小时结束后处理这一小时的有效数据。

以天为维度的批处理问题在于输入数据的变化只有在一天后才会对输出的结果有影响，这对很多急性子的用户来说太慢了。为了减少延迟，我们可以更频繁的执行批处理程序-例如每一秒都执行一次来处理上一秒的有效数据，甚至于说连续的，完全不划分固定区间，每个事件来临就立即处理。这便是「流处理」（*stream processing*）背后的思想。

通常，“流”指的是随着时间推移而逐渐可用的数据。这个概念出现在很多地方：在Unix的标准输入（stdin）和标准输出（stdout）中，编程语言（lazy lists）\[[](#ch12References2)],文件系统API（例如Java的FileInputStream），TCP连接，通过互联网传输的音频和视频等等。

本章，我们将把事件流（event streams）视为一种数据管理机制：一种对应于我们上一章所介绍的，一种无界的，持续增量处理的批数据处理方式。我们首先会讨论流在网络中怎样表示、存储和传输。在“[数据库与流](#database_streams)”中我们会讨论流和数据库之间的联系。最后，在“[流处理](#processing_streams)”中,我们会继续探讨处理这些流的方法和工具，以及它们用来构建应用的方式。

## 传输事件流

在批处理的世界，任务的输入输出一般是文件（可能是分布式的文件系统）。那么流又是怎样的呢？

当输入是文件（字节序列）时，首先要做的处理步骤通常是把它解析成为记录序列。在流处理的上下文中，一条记录通常会被称为**事件**（event），本质上它们是同一件事物：一种小的、独立的、不可变的含有事物发生时间节点信息的对象。一个事件通常会包含一个表明在一天中发生的时钟节点一致的时间戳（详见[单调和实时时钟](#monotonic_versus_clocks)“Monotonic Versus Time-of-Day Clocks”）。

举个例子，这个发生的事件既可能是来自于用户执行的一个操作，如浏览网页或者支付一笔账款。也可能是来自于机器，如对温度进行的周期性测量或者是CPU的利用率指标。在[使用Unix工具的批处理](#bp_with_unix_tool)这个例子中，web服务器的每一行日志便是一个事件。


事件的编码方式可能是字符串，或者是JSON，也可能是我们在[第五章](#chapter5)中讨论过的二进制表格。怎样编码决定了你可以怎样存储你的事件，例如你可以把它追加到文件中，也可以把它插入到关系型数据库表，又或者是把它写入到文档数据库中。另外你还可以通过网络把事件发送给其他节点来处理。

在批处理中，一次写入文件可能会被多个任务读取。类似的，在流处理的语境下，一个事件也是由生产者一次生成（也可称为发布者或发送者），然后可能被多个消费者消费（订阅者或者是收件人）\[[3](#anchor-3)]。在文件系统中，文件名用以标识一组相关记录；在「流处理」系统中，相关的事件通常会组合到**主题**（topic）或者**数据流**（stream）中。

原则上，文件或数据库足以连接生产者和消费者：生产者将它生产的每个事件写入到数据存储，然后每个消费者定期的从数据存储中轮询数据来检查从上次拉取后产生的新事件。实际上这正是批处理在每天结束时处理这一天有效数据的过程。

但是，若数据存储不是专门为这种用途而设计的，要在低延迟的情况下实现持续处理数据，轮询的代价是非常大的。你轮询操作越频繁，返回新事件的请求比例越低，系统开销也就越大。因此，当新事件产生时，我们最好主动通知消费者（而不是消费者轮询）。

传统数据库通常不能很好的支持这种通知机制：关系型数据库一般使用**触发器**（*triggers*），来承接这种变化（比如：往表里插入了一条数据），但是它们的能力非常有限，这更像是数据库事后的补偿设计\[[4](#ch12-References4)]。所以，它是一种味了支撑事件通知而开发的特殊工具。

## 消息系统








