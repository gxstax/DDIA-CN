![ch5](../img/chapter5.png)

# 第五章：备份

> 一件事情出错和不出错最大的区别在于当一件不可能出错的事情出错时，通常已经难以挽救了。
>
> — Douglas Adams, *Mostly Harmless* (1992)



备份意味着要在被网络连接的多个不同的机器之间存储相同的数据。正如在 [第二部分](README.md) 引言中所描述的，有以下几点原因是为什么需要进行备份：

* 使数据在地理位置上离用户更近（减少延迟）。
* 当部分数据存储服务宕机后，系统仍然可以继续提供服务（提高可用性）。
* 自由伸缩提供读取服务的机器数量（提高读取吞吐量）。

在本章我们假设你的数据量足够小，每台机器都可以完整的保存整个数据集。在[第六章](chapter6.md)会抛开这个假设来谈谈对于单台机器来说数据量过大时怎么来对数据集进行分区（分片）。在后续的章节，我们还会讨论在备份数据系统中可能产生的一些错误或者缺陷，以及如何来解决它们。

如果你的副本数据不随着时间而发生变化，那复制是比较容易的：你只需要一次性的把数据复制到各个节点就可以了。复制机制中所有的难点都在于对更改的数据处理上，这也是我们本章所探讨的重点。我们会谈到节点之间对于变更数据复制的3种主流的算法：单主（*single-leader*）复制，多主（*multi-leader*）复制以及无主（*leaderless*）复制。所有对于分布式数据的处理无外乎这3种方式。他们之间各有利弊，接下来我们将会详细的阐述。

复制（replication）涉及许多权衡考虑，例如是否使用同步或异步复制，以及如何处理失败的副本。这些通常是数据库中的配置选项，尽管具体细节因数据库而异，但原则上他们的实现是相似的。我们将在本章中讨论此他们做出这些选择的原因。

数据库的复制是一个比较久远的话题—自从19世纪70年代以来，它的原理性的东西并没有太大变化[[1](#ch5References1)]，因为底层网络的「**约束规范性条件（fundamental constraints ）**」也一直保持没变。然而，在研究之外，很多开发人员长时间以来一直认为数据库始终都只有一个节点。对于分布式数据的主流应用是近期才出现的。由于很多应用开发人员都是刚接触到这一领域，所以他们对于像「最终一致性（*eventual consistency*）」这些概念存在很多误解。在[延迟同步问题](#延迟同步问题)我们将更深入的叙述「最终一致性」，并且会讨论诸如「**读己写-*read-your-writes***」和「**单调读 -*monotonic reads***」等概念。

## 领导者和追随者

每一个存储数据库拷贝的节点我们称之为一个「**副本-*replica***」。随着多副本的产生，问题也会随之而来：我们怎样确保每个副本中都保存有完整的数据？

每一次对数据库的写请求必须能被所有副本接收并且处理；不然，数据库副本之间就可能会包含不同的数据。对于此类问题最常用的解决方案是如[<font color="#A7535A">**图5-1**</font>](#figure5-1)所示的「**基于领导者复制-*leader-based replication***」（也被称作「**主动/被动-*active/passive***」 或者「**主从复制-*master–slave replication***」）。它的工作机制如下：

1. 把其中一个副本设定为「**领导者**」（也被称为「**主-*master***」或「**首要-*primary***」）。如果客户端对数据库写入数据，则必须将他们的请求发送给领导者，领导者会最先把新写入的数据写入到本地。

2. 其它的副本就是我们所说的「**追随者**」（也被叫做***read replicas***，***slaves***，***secondaries***或 ***hot standbys***）。[^i]当领导者把新数据写入本地存储后，还会把变更的数据以变更日志或变更流的形式发送给它的追随者。追随者接受到领导者发送的变更日志后便会更新它本地的数据库备份数据，它会按照领导者的执行顺序来执行本地的变更。

   ---

   [^i]:Different people have different definitions for *hot*, *warm*, and *cold* standby servers. In PostgreSQL, for example, *hot standby* is used to refer to a replica that accepts reads from clients, whereas a *warm standby*processes changes from the leader but doesn’t process any queries from clients. For purposes of this book, the difference isn’t important.

3. 当客户端读取时，它既可以从领导者副本读，也可以从追随者副本读取。但是写入请求只能由领导者副本接收（从客户端的角度来看，追随者副本是只读的）。

![图5-1](../img/figure5-1.png)

<a id="figure5-1"><font color="#A7535A">**图5-1.**</font></a> **基于领导者（主从）复制**

这种复制模型是很多诸如（PostgreSQL (9.0以后版本), MySQL, Oracle Data Guard [[2](#ch5References2)], 以及 SQL Server的

AlwaysOn Availability Groups [[3](#ch5References3)]）等关系型数据库内建特性。同样也被用在包括MongoDB，RethinkDB和Espresso[[4](#ch5References4)]等非关系型数据库中。最后，基于领导者复制并不局限于数据库：像一些分布式消息系统如Kafka[[5](#ch5References5)]和RabbitMQ的高可用队列[[6](#ch5References6)]也是用它来实现的。同样的，一些网络文件系统和DRBD这样的块设备复制也可以用它来实现。

### 同步复制与异步复制

副本系统一个最重要的细节就是复制是同步进行还是异步进行。（在关系型数据库中，这通常是一个配置项；其它的系统通常会硬编码为其中的一种。）

回想一下[<font color="#A7535A">**图5-1**</font>](#figure5-1)的用户更新头像的场景。在某一时间点，客户端发送更新请求到领导者副本；随后，leader副本接收到请求，然后在合适的节点把变更数据转发给追随者。最后，leader副本通知客户端数据更新成功。

[<font color="#A7535A">**图5-2**</font>](#figure5-2) 展示了系统各组件之间的交互流程：客户端，leader副本，2个追随者副本。时间从左到右。箭头表示请求或响应信息。

![图5-2](../img/figure5-2.png)

<a id="figure5-2"><font color="#A7535A">**图5-2.**</font></a> **基于领导者复制，一个同步复制追随者，一个异步复制追随者。**   

在[<font color="#A7535A">**图5-2**</font>](#figure5-2)的例子中，**追随者1**采取的是同步复制：领导者会一直等待，直到接收追随者1确认已经接收到写数据的消息后，才会给用户返回成功消息，在这之前数据对其它客户端是不可见的。**追随者2**采用的是异步复制：领导者发送变更消息后并不会等追随者的确认消息而直接返回。

图中所展示的是**追随者2**在处理数据时存在比较大的延迟。通常情况下，复制是比较快的：大多数数据库系统的追随者一秒钟内就会处理完变更数据的同步。但是，追随者的处理时间并不受我们控制。在某些情况下，追随者可能会落后领导者几分钟甚至更久。例如，追随者刚好是在执行故障恢复，系统刚好处在最大负载值，或者节点之间刚好有网络故障。

同步复制的优势在于追随者始终能够保证和领导者的数据保持一致。当领导者突然宕机，我们仍能够保证追随者副本的数据依旧是有效的。缺点是当同步复制的追随者无响应时（可能发生崩溃，网络错误，或其它的一些原因），写进程将无法继续执行。领导者必须阻塞所有的写操作，直到同步复制重新恢复。

基于这个原因，那么把所有的追随者都设置为同步复制就会显得很不切实际了：因为任何一个节点的中断，都会导致整个系统逐渐停止。实际上，如果允许数据库有同步复制，通常是让其中一个追随者采用同步复制，剩余的追随者则采用异步复制。如果采用同步复制的追随者不可用或复制变慢，那么从异步复制的追随者中挑选一个变为同步复制。这样我们就能够保证至少有两个副本保存有最新的数据：领导者副本和采用同步复制的追随者副本。这种配置机制，我们有时称它为「**半同步-*semi-synchronous***」[[7](#ch5References7)]。

通常，基于领导者复制会被配置称完全同步的。在这种场景下，如果领导者副本发生宕机并且无法恢复时，所有未复制完成的从节点将会丢失这部分写操作数据。这就意味着写入操作不保证能成功的持久化，即使客户端收到了服务端的确认响应。然而，一个完全异步化的配置也有其优势，那就是即使所有的从节点数据同步都落后与领导者副本，也依然不会影响领导者副本的写入。

像「异步复制」这种弱持久性的设计咋看是一个糟糕的折衷办法，但它仍被广泛的应用，尤其是在从节点分布在不同的地理位置这种场景中。我们在[延迟同步问题](#延迟同步问题)这一小节再回过头来讨论这个问题。

---

<center><font face="宋体" size="4" color=black>副本机制探究</font></center>

<font size=2>对于异步复制系统来说，因为领导者副本出错而导致的数据丢失是一个很严重的问题，所以研究人员依然在寻找一种复制方法，既能保证数据不丢失，又能保持很好的性能和可用性。例如，「链式复制-*chain replication* 」[[8](#ch5References8),[9](#ch5References9)] 这种同步复制的变体，就已经在成功的在微软Azure Storage系统中加以应用。</font>

<font size=2>多副本一致性与共识之间存在很强的联系（多个副本之间值达成一致性），我们将在[第九章](chapter9.md)详细讨论这一点。本章我们主要研究的重点是数据库事件中比较简单通用的副本机制。</font>

---



### 配置新的从节点

当你需要增加副本的数量或者替换异常的节点时，就需要考虑增加新的从节点。那我们又如何来保证新加入的从节点数据和主节点的数据保持一致的呢？

如果还是单纯的把数据从一个节点复制到另一个节点通常是不够的，因为客户端会不断的往数据库中写入数据，这就导致数据始终处于不断变化之中，因此常规的文件拷贝方式将会导致不同的节点上呈现出不同时间点的数据，这种情况往往不是我们愿意看到的。

当然你可以锁定数据库（使其不可写）来使得磁盘上的文件保持一致，但使这会违反「高可用」的设计原则。但好在我们可以在不停机的情况下来设置一个从节点。它的执行步骤如下：

1. 在某一时间点对主节点的数据打一个一致性快照。这样可以避免长时间的锁定整个数据库。现在大多数数据库都支持此特性，同样快照也是系统备份所必需的。某些情况下，可能需要借助第三方工具，比如MySQL的*innobackupex*[[12]](#ch5References12)。

2. 复制镜像数据到新的从节点。
3. 从节点连接主节点并请求在同步快照中间所发生的变更数据。这要求快照能够精确的标识出它在复制日志的具体位置。这个位置有多种不同的称呼：如PostgreSQL将其称为「**日志序列号-*log sequence number***」，MySQL称它为「*binlog coordinates*」。
4. 当从节点同步完快照数据后，再处理变更的积压日志的过程，我们称之为追赶。它将继续处理主节点接下来发生的数据变更。

创建从节点的步骤也会因为数据库的不同存在着很大的差异。在某些系统中，这些处理过程完全是自动化的，而在另外一些系统中工作流程又相对会比较复杂，有时甚至需要管理员手动操作。

### 节点宕机处理

系统中的任何一个节点都可能因为故障或计划中系统维护（例如节点重启或安装内核安全补丁）发生宕机，

如果能在不停机的情况下重启节点，将会给运维带来巨大的便利。因此，我们的目标就是在单个节点宕机的情况下，也要保证系统整体可运行，并把节点宕机的影响降至最小。

那么，如何基于领导者复制来保证高可用呢？

#### 从节点失效：追赶式恢复

每个从节点在本地磁盘都维护一份从主节点接收到的变更数据的日志记录。如果从节点崩溃恢复，或者主从节点之间网络暂时中断，从节点也可以轻松的恢复，因为通过本地日志，从节点能够很快查询到在崩溃发生时所处理到的最后一个事务。因此，当从节点重新连上主节点时，再重新请求从崩溃那一刻到恢复期间所有的变更数据就可以了。当把所有的变更数据都同步完成后，便可以像之前一样继续接受来自主节点的数据变更流就可以了。

#### 主节点失效：故障转移

主节点失败处理则会比较棘手：首先要选出一个从节点变更为主节点，客户端要重新配置他们的写请求到新的主节点，其余的从节点也要重新从新的主节点消费变更数据。这个过程便是「**故障转移**」。

故障转移可以切换，主动的通知管理员主节点失效，管理员则会采取一些必要的措施来创建一个新的主节点。也可以自动进行，自动切换的步骤通常如下所示：

1. ***确认主节点失效。***

   导致主节点失效的原因有很多，比如系统崩溃，断电，网络故障等等。目前没有一个万全之策能够精确的检测出到底哪里出现了问题，所以大多数系统都会采用超时机制：节点之间频繁的发送心跳检测信息，如果发现某一节点在较长的一段时间里（例如30s）没有应答，则认为该节点失效（计划中的主节点主动下线维护，不在此考虑范围内）。

2. ***选举新的主节点。***

   可以通过选举的方式在剩余的副本中选出一个作为主节点，或者从之前事先推举的副本中挑选出一个作为主节点。最好的领导者候选人一般是从原有主节点同步数据最多的副本（这样可以最大限度的减少数据丢失）。让所有节点同意某一副本为主节点是典型的共识问题，我们将在[第九章](chapter9.md)详细的讨论。

3. ***重新配置系统配置信息来使新的主节点生效。***

   客户端需要将新的写请求发送给新的主节点（我们在会在[请求路由](chapter6.md#请求路由)这部分讨论）。如果原来的主节点重新上线，它可能并不能感知到其他副本已经强制使其下线而仍然认为自己是主节点。这时候系统要能够确保旧的主节点变为从节点，并能够知晓哪一个是主节点。

   

但是，上述切换过程存在着诸多变数：

* 如果使用异步复制，在旧主节点故障前，新的主节点可能还没有接受到所有来自旧主节点的写入数据。如果等到新的主节点被选举成功后，旧的主节点又重新加入到集群中，那么旧主节点中原有的写入数据将怎么处理呢？同时新的主节点也可能会接收到冲突的写请求数据（旧主节点意识不到自己已经不是主节点，从而还会向其它节点发送同步数据）。针对这种情况，通常的解决方案是原来旧的主节点简单的丢弃掉未复制成功的这部分写数据，但这可能违背客户端的持久化原则。
* 如果有外部的存储系统依赖于数据库的内容做协调数据，那么丢弃写数据将会是特别危险的操作。例如，GitHub的一次事故中[[13](#ch5References13)]，某个未完全同步完成的从节点被晋升为主节点，数据库使用了自增数作为新增行的主键，但是由于新主节点的自增数要落后于原来旧的主节点，所以复用了部分旧主节点已经分配过的数值作为主键。刚好这些主键也被用在了Redis中，造成了MySQL于Redis中的主键不一致问题，从而导致一些私人的数据信息泄漏给了其它的用户。
* 在某些错误的场景下（参见[第八章](chapter8.md)），可能会发生两个节点都认为他们是主节点的情况。这种场景称之为「**脑裂**」，这其实是相当危险的：如果两个主节点都能接收写请求，但是没有处理冲突（参见：[多主复制](#多主复制)），那数据极有可能丢失或损坏。出于安全的应急方案考量，某些系统会制定一些机制，就是当检测到有两个主节点存在时，便会强制关闭其中的一个。[^ii]当然，如果这种机制设计时考虑的不够周全，也可能会出现两个节点都被关闭的情况[[14](#ch5References14)]。
* 如果设置合适的超时时间来限定主节点的失效状态呢？如果设置的超时时间过长，那么意味着一旦出现主节点失效的情况，恢复所需要的时间也会更长。但是，如果设置的超时时间太短，则会导致不必要的故障转移发生。例如，突发的负载峰值可能会导致节点的响应时间超出所设定的超时时间，或者网络故障也可能会导致数据包的延迟。如果系统已经处在负载峰值或网络故障的情况下，一个不必要的故障转移不会使系统变好，反而变得更糟。

对于这些问题没有很好的解决办法。基于这些原因，一些运维团队宁愿选择手动执行故障切换，也不愿意采用自动故障转移的方案。

节点失效，网络不稳定，副本之间对一致性，持久性，可用性和延迟所做的一些妥协措施等等这些问题，实际上都是分布式系统中存在的基本性问题。在[[第八章](#chapter8.md)]和[[第九章](#chapter9.md)]中我们会更加深入的来讨论。

---

[^ii]: This approach is known as *fencing* or, more emphatically, *Shoot The Other Node In The Head* (STONITH).We will discuss fencing in more detail in [“The leader and the lock”](chapter8#主节点和锁) 。



### 同步日志实现

主从复制到底时如何工作的呢？实践中有多种不同的实现方法，让我们先简单的看一下他们大概的实现原理。

#### 基于语句复制

最简单的情况，主节点记录它执行过的每一次写请求（操作语句），然后把操作语句的日志发送给从节点。对于关系型数据库来说，这意味着每一个插入、更新、删除操作原语都会转发给从节点，然后从节点就好像他们是客户端发过来的请求一样，解析并执行这些SQL操作原语。

尽管这听起来很合理，但这种直接使用操作语句复制方式在下面这些场景中就显得不太适用了：

* 一些**非确定性函数**的调用，如获取当前日期和时间函数-NOW() ，和获取随机数函数- RAND() ，在不同的副本中可能会生成不同的值。
* 如果语句中涉及自增列的操作，或者依赖于数据库现有的数据（例如：UPDATE … WHERE *<条件>*），则要求它们在每个副本中的执行顺序要高度一致，否则就可能会产生不同的影响。这其实会对并发事务处理产生诸多限制。
* 一些会产生副作用的语句（例如：触发器，存储过程，自定义函数），除非这些这些语句产生的结果是可控的，否则在不同的副本中就会产生不同的副作用。

这些问题都是可能被解决的，例如，当操作语句被日志化的时候，主节点可以用固定的返回值来替代这些不确定函数，这样，从节点接收到的便都是相同的值了。但是，这里面有太多的边界条件需要考虑，所以现在通常都是使用其它的复制方案。

MySQL在5.1版本之前使用的是基于语句的复制方案。由于这种方案的逻辑和日志存储十分紧凑，所以在现在仍然在某些地方使用。但是，默认情况下，如果有不确定的源于，MySQL会采用「**基于行复制-row based replication**」（稍后讨论）的方案。VoltDB则仍然还在使用基于语句的复制，它是通过事务级别的确定性来保证数据复制的安全。[[15](#ch5References15)]

#### 预写日志（WAL）传输

在[[第三章](../part1/chapter3.md)]中，我么讨论了存储引擎是怎么在磁盘中存储数据的，我们发现通常每个写操作都是以追加写的形式记录到日志中的：

* 对于日志结构存储引擎（参见：[SSTables和LSM-Trees](../part1/chapter3.md#SSTables和LSM-Trees)）,日志是主要的存储方式。后台进程会对日志段进行压缩和回收。
* 对于采用覆盖写磁盘的B-tree（参见：[B-Trees](../part1/chapter3.md#B-Trees)）结构,每次写操作之前都会预先写入日志，如果系统发生崩溃，通过索引就能快速的恢复到和此前的一致性状态。

不管是哪种情况，所有对数据库写入的字节序列都会被记录到日志中。因此可以使用完全相同的日志在另外一个节点上构建新的副本（除了将日志写入磁盘意外，主节点还可以通过网络将其发送给从节点）。

当从节点处理完这些日志后，那么它将拥有和主节点完全一致的数据结构内容。

像PostgreSQL，Oracle和其它一些系统都支持这种复制方式[[16](#ch5References16)]。但是它主要的缺点在于日志描述的数据非常底层：WAL日志通常包含的是哪些磁盘中的block中的哪些字节发生的变动，这便会造成复制机制和存储引擎之间的耦合。如果数据库的数据存储格式发生变动，那么同一个份数据日志将不支持在不同版本的主节点和从节点之间运行。

这看起来只是一个细微实现的变化，但却会对维护产生较大的影响。如果我们采用的复制协议允许从节点可以使用与主节点不同的软件版本，那么你就可以先升级你的从节点系统版本，然后在执行故障转移是其中一个升级后的从节点变为新的主节点，从而实现不停机升级。但是如果复制协议在不同的系统版本之间不能兼容，那么升级就必须要停机处理了。

#### 基于行的逻辑日志复制

另外一种方式是复制日志和存储引擎采用不同的日志格式，这样就可以使得复制日志和存储引擎之间解耦。这种用以和存储引擎（物理上的）数据表示相区分的复制日志称之为「**逻辑日志**」。

关系型数据库的逻辑日志通常是对数据库表中行维度的一系列写操作的记录描述：

* 对于插入行，日志会记录所有列的值。
* 对于删除行，日志中记录能够唯一标识出被删除行的信息。通常情况下都是主键，但如果表中没有主键，那么就需要记录被删除的所有列的值。
* 对于更新行，日志也要记录能够唯一标识出被更新行的所有信息以及每一列更新的新值（或者被更新列的最新值）。

如果一个更新多行的事务，除了会产生上述多条这样的日志记录外，还会跟上一条标记该事务已经提交的记录。MySQL的binlog日志(当配置了基于行复制格式时)就是使用这种方式[[17](ch5References17)]。

正是由于做到逻辑日志和内部存储引擎的解耦，我们就能很容易的做到向后兼容性，从而使得主节点和从节点可以运行不同版本的系统，甚至是不同的存储引擎。

另外逻辑日志也更容易被外部应用系统解析。如果你想要把内容发送到外部系统（例如一个用于离线分析的数据仓库），或者是构建自定义索引和缓存，就显得特别有优势了。这种技术也被称之为「**变更数据捕获-*change data capture***」，我们在[第11章](../part3/chapter11.md)中继续讨论。

#### 基于触发器的复制

到目前为止，我们所讨论的所有复制机制都是由数据库系统自行完成的，这其中并不涉及任何的应用程序代码。这在很多通用场景中都是我们乐意看到的情况，但是也会存在一些场景需要我们我们的系统具备更高的灵活性。例如你只想备份数据的一部分数据，或者想把一种数据库中的数据备份到另外一种数据库中，又或者你需要冲突解决逻辑（参见：[写冲突处理](#写冲突处理)），这时候你必须要把复制交由应用层来实现。

像Oracle的GoldenGate[[19](#ch5References19)]这样的工具，可以通过读取数据库日志的方式让应用程序获取到数据变更。另外就是可以利用关系性数据库本身的特性：「**触发器-*triggers***」和「**储存过程-*stored procedures***」。触发器支持注册应用层代码到数据库系统中，当有数据变更（写事务）发生时系统可以自动的执行你注册的代码。我们就可以利用这个触发器把变更的日志信息记录到一个单独的表中，然后让外部的处理程序读取该表，来实现我们在应用层的自定义逻辑，或者把变更的数据备份到其它的系统中。Oracle的Databus[[20](#References20)]和Postgres的Bucardo[[21](#References21)]就是这种技术的典型代表。

不过，基于触发器的复制通常比其它形式的复制方式开销更高，也比数据库内置的复制更容易出错。但是，它所拥有的高度灵活性使得他仍然又它的用武之地。

## <a id="ReplicationLag">延迟同步问题</a> 

容忍节点故障仅仅是复制的其中一个原因。正如[第二部分](READEME.md)大纲所介绍的，其它原因还包括「可扩展性」（想比单台机器处理更多的请求）和「低延迟性」（将服务部署在物理地址离用户更近的地方）。

「**主从复制**」要求所有的写请求都经过主节点，但是只读查询则可以通过任一副本来进行。对于读多写少（Web应用模式）的工作模式来说，这是一个不错的选择：创建多个从节点，把读请求分发到这些从节点上。减轻主节点工作负载同时又能实现用户的就近访问。

在这种扩展机制下，你可以仅通过简单的增加从节点就能够达到提高读请求吞吐量的目的。但是这种方法仅适用于异步复制，如果你试图对所有的从节点都采用同步复制的方式，那么一个节点的失效或网络故障就将会导致整个系统不可写。而且，你的节点越多，发生这种状况的概率越高，所以，一个完全同步复制的配置可靠性是非常差的。

但是，如果应用从一个异步同步的副本中读取数据，又可能会因为从节点落后主节点太多而导致读取到的数据是过时的。这就会导致数据库中出现明显的不一致现象：如果你在同一时间分别在主节点和从节点执行相同的查询，你可能会得到不同的结果，因为并不是所有的写操作都反应到了从节点上。不过这个不一致现象也只是暂时的：如果你停止写入等待一会，从节点最终会赶上并与主节点数据一致。这个效应就是我们所说的「**最终一致性-*eventual consistency***」[[22](#ch5References22),[23](#ch5References23)]。[^iii]

”最终-eventually“一词其实是一个模糊的概念：通常，一个副本的落后时间并没有一个限制。通常，主节点和从节点完成同一个写操作之间的时间延迟（**同步延迟-*replication lag***）可能不足一秒，像这样的段时间延迟，通常对我们的服务是无感的。但是，如果系统已经达到负载极限或者网络出现问题，那么延迟时间便会很容易的达到几秒甚至是几分钟。

当延迟时间过长时，由它所带来的不一致问题就不仅仅是理论层面的问题了，而是它实实在在的对我们的应用产生影响。在本节中，我们会重点介绍三种「**同步延迟**」的问题，并给出相应的解决方案，

---

[^iii]:  「最终一致性」一词最早由Douglas Terry 提出. [[24](#ch5References24)]，后来经由 Werner Vogels[[22](##ch5References22)]而普及，并称为很多NoSQL项目的标志性口号。但事实上，采用异步复制的关系性数据库同样具有该特性。



### 读己写

很多应用能够使得用户提交一部分数据后，接下来便可以看到他们之前已经提交的数据。这可能存在于用户数据库或者对一个主题帖子的评论中。当新的数据被提交后，它必须先发送到主节点，但是当用户访问这些数据时，它可能是从某一从节点读取的。尤其是在数据读取密集型应用场景中更是如此。

如[<font color="#A7535A">**图5-3**</font>](#figure5-3)所示，在异步复制中存在这样一个问题：如果用户是在写后不久便访问该数据，那么新写入的数据可能还没来得及同步到从节点。对于用户来说，看起来好像他提交的数据丢失了，这对用户来说显然是不可接受的。

![](../img/figure5-3.png)

<a id="figure5-3"><font color="#A7535A">**图5-3.**</font></a> **用户写之后紧接着伴随着一个读操作，为了防止这种异常现象，我们就需要「写后读一致性」。**

在这种场景下，我们就需要「**写后读一致性-*read-after-write consistency***」，也被称作「**读写一致性-*read-your-writes consistency***」[[24](#ch5References4)]。这样就能够保证用户每次刷新页面，都能够看到他们最新提交的内容。不保证其它用户操作的一致性：其它用户的更新操作可能会到晚些时候才能看到。但是，它保证了用户自己的输入一定能够正确的保存。

那么基于「**主从复制**」的系统中怎么来实现「**写后读一致性**」的呢？其实有很多可行性方案，下面来列举一二：

* 当用户读取哪些可能会被修改到的数据时，则从主节点读取，否则，就从从节点读取。这就要求我们需要一种方法来知道你查询的数据是否有可能会被修改到。例如社交网站的用户个人信息通常只能狗被用户自己编辑，而其他人时无权限修改的。所以，我们就可以简单的规定：总是从主节点查询自己的配置信息，而其他用户的配置信息则从从节点读取。
* 如果应用的大部分数据都有可能会被修改到，那么上述方法可能不太适用，因为这样就会导致大部分的数据都需要从主节点来读取（这会破坏读操作的可扩展性）。因此，我们就可能会考虑其他的方案来决定是否从主节点读取数据。例如，你可以追踪最后一次更新数据的时间，在最后一次更新后的一分钟内，我们都让它从主节点读取。你也可以监控从节点同步的滞后时间值，来避免从滞后大于一分钟的从节点读取数据。
* 客户端可以记录最近一次更新的时间戳，这样系统就能够确保至少在这个时间戳之前副本提供的读取服务都是有效的。如果该副本的数据不够新，要么由另外一个副本提供读取服务，要么等到该副本追上主节点后再执行查询请求。这个时间戳可以是「**逻辑时间戳-*logical timestamp***」（例如用以标识写入顺序的日志序列编号）或者是实际系统时钟（这种情况，时钟同步是关键因素，详见：[第八章：不可靠时钟](chapter8.md)）。
* 如果副本分布在多个不同的数据中心（例如为了高可用，把副本放置在离用户地理位置更近的地方），这种情况会更复杂。任何需要经过主节点处理的请求都必须要路由到主节点所在的数据中心。

如果用户从不同的设备访问服务，例如一个web桌面浏览器和一个移动APP，这又会增加一定的复杂度。这种情况可能需要提供一种跨设备写后读一致性保证：如果用户在一台设备上输入了一些信息，然后在另外一台设备上访问，要保证用户在另外一台设备上能看到他刚才的输入信息。

这就有一些值得我们思考的问题了：

* 记录用户上次更新的时间戳是一件比较困难的事，因为跑在其中一台设备的程序是不知道另外一台设备有更新发生的。这可能需要数据集中化处理。
* 如果我们的副本是跨多个数据中心分布的，我们没有办法保证来自不同设备的连接信息经过路由后到达同一个数据中心（例如，用户的台式机用的是家庭宽带，而移动设备使用的蜂窝网络，设备之间的网络路由可能完全不同）。如果要从主节点读取数据，那么你必须要把用户的所有请求都路由到同一个数据中心。

### 单调读

在前面





### 一致性前缀读

### 同步延时解决方案



## 多主复制

### 多主复制用例

### 写冲突处理

### 多领导者副本拓扑



## 无主复制

### 写时节点故障

### 限定法定数一致性

### Sloppy Quorums and Hinted Handoff



## <a id="detectiongConcurrentWrites">并发写入检测</a>
