![ch5](../img/chapter5.png)

# 第五章：备份

> 一件事情出错和不出错最大的区别在于当一件不可能出错的事情出错时，通常已经难以挽救了。
>
> — Douglas Adams, *Mostly Harmless* (1992)



备份意味着要在被网络连接的多个不同的机器之间存储相同的数据。正如在 [第二部分](README.md) 引言中所描述的，有以下几点原因是为什么需要进行备份：

* 使数据在地理位置上离用户更近（减少延迟）。
* 当部分数据存储服务宕机后，系统仍然可以继续提供服务（提高可用性）。
* 自由伸缩提供读取服务的机器数量（提高读取吞吐量）。

在本章我们假设你的数据量足够小，每台机器都可以完整的保存整个数据集。在[第六章](chapter6.md)会抛开这个假设来谈谈对于单台机器来说数据量过大时怎么来对数据集进行分区（分片）。在后续的章节，我们还会讨论在备份数据系统中可能产生的一些错误或者缺陷，以及如何来解决它们。

如果你的副本数据不随着时间而发生变化，那复制是比较容易的：你只需要一次性的把数据复制到各个节点就可以了。复制机制中所有的难点都在于对更改的数据处理上，这也是我们本章所探讨的重点。我们会谈到节点之间对于变更数据复制的3种主流的算法：单主（*single-leader*）复制，多主（*multi-leader*）复制以及无主（*leaderless*）复制。所有对于分布式数据的处理无外乎这3种方式。他们之间各有利弊，接下来我们将会详细的阐述。

复制（replication）涉及许多权衡考虑，例如是否使用同步或异步复制，以及如何处理失败的副本。这些通常是数据库中的配置选项，尽管具体细节因数据库而异，但原则上他们的实现是相似的。我们将在本章中讨论此他们做出这些选择的原因。

数据库的复制是一个比较久远的话题—自从19世纪70年代以来，它的原理性的东西并没有太大变化[[1](#ch5References1)]，因为底层网络的「**约束规范性条件（fundamental constraints ）**」也一直保持没变。然而，在研究之外，很多开发人员长时间以来一直认为数据库始终都只有一个节点。对于分布式数据的主流应用是近期才出现的。由于很多应用开发人员都是刚接触到这一领域，所以他们对于像「最终一致性（*eventual consistency*）」这些概念存在很多误解。在[延迟同步问题](#延迟同步问题)我们将更深入的叙述「最终一致性」，并且会讨论诸如「**读己写-*read-your-writes***」和「**单调读 -*monotonic reads***」等概念。

## 领导者和追随者

每一个存储数据库拷贝的节点我们称之为一个「**副本-*replica***」。随着多副本的产生，问题也会随之而来：我们怎样确保每个副本中都保存有完整的数据？

每一次对数据库的写请求必须能被所有副本接收并且处理；不然，数据库副本之间就可能会包含不同的数据。对于此类问题最常用的解决方案是如[<font color="#A7535A">**图5-1**</font>](#figure5-1)所示的「**基于领导者复制-*leader-based replication***」（也被称作「**主动/被动-*active/passive***」 或者「**主从复制-*master–slave replication***」）。它的工作机制如下：

1. 把其中一个副本设定为「**领导者**」（也被称为「**主-*master***」或「**首要-*primary***」）。如果客户端对数据库写入数据，则必须将他们的请求发送给领导者，领导者会最先把新写入的数据写入到本地。

2. 其它的副本就是我们所说的「**追随者**」（也被叫做***read replicas***，***slaves***，***secondaries***或 ***hot standbys***）。[^i]当领导者把新数据写入本地存储后，还会把变更的数据以变更日志或变更流的形式发送给它的追随者。追随者接受到领导者发送的变更日志后便会更新它本地的数据库备份数据，它会按照领导者的执行顺序来执行本地的变更。

   ---

   [^i]:Different people have different definitions for *hot*, *warm*, and *cold* standby servers. In PostgreSQL, for example, *hot standby* is used to refer to a replica that accepts reads from clients, whereas a *warm standby*processes changes from the leader but doesn’t process any queries from clients. For purposes of this book, the difference isn’t important.

3. 当客户端读取时，它既可以从领导者副本读，也可以从追随者副本读取。但是写入请求只能由领导者副本接收（从客户端的角度来看，追随者副本是只读的）。

![图5-1](../img/figure5-1.png)

<a id="figure5-1"><font color="#A7535A">**图5-1.**</font></a> **基于领导者（主从）复制**

这种复制模型是很多诸如（PostgreSQL (9.0以后版本), MySQL, Oracle Data Guard [[2](#ch5References2)], 以及 SQL Server的

AlwaysOn Availability Groups [[3](#ch5References3)]）等关系型数据库内建特性。同样也被用在包括MongoDB，RethinkDB和Espresso[[4](#ch5References4)]等非关系型数据库中。最后，基于领导者复制并不局限于数据库：像一些分布式消息系统如Kafka[[5](#ch5References5)]和RabbitMQ的高可用队列[[6](#ch5References6)]也是用它来实现的。同样的，一些网络文件系统和DRBD这样的块设备复制也可以用它来实现。

### 同步复制与异步复制

副本系统一个最重要的细节就是复制是同步进行还是异步进行。（在关系型数据库中，这通常是一个配置项；其它的系统通常会硬编码为其中的一种。）

回想一下[<font color="#A7535A">**图5-1**</font>](#figure5-1)的用户更新头像的场景。在某一时间点，客户端发送更新请求到领导者副本；随后，leader副本接收到请求，然后在合适的节点把变更数据转发给追随者。最后，leader副本通知客户端数据更新成功。

[<font color="#A7535A">**图5-2**</font>](#figure5-2) 展示了系统各组件之间的交互流程：客户端，leader副本，2个追随者副本。时间从左到右。箭头表示请求或响应信息。

![图5-2](../img/figure5-2.png)

<a id="figure5-2"><font color="#A7535A">**图5-2.**</font></a> **基于领导者复制，一个同步复制追随者，一个异步复制追随者。**   

在[<font color="#A7535A">**图5-2**</font>](#figure5-2)的例子中，**追随者1**采取的是同步复制：领导者会一直等待，直到接收追随者1确认已经接收到写数据的消息后，才会给用户返回成功消息，在这之前数据对其它客户端是不可见的。**追随者2**采用的是异步复制：领导者发送变更消息后并不会等追随者的确认消息而直接返回。

图中所展示的是**追随者2**在处理数据时存在比较大的延迟。通常情况下，复制是比较快的：大多数数据库系统的追随者一秒钟内就会处理完变更数据的同步。但是，追随者的处理时间并不受我们控制。在某些情况下，追随者可能会落后领导者几分钟甚至更久。例如，追随者刚好是在执行故障恢复，系统刚好处在最大负载值，或者节点之间刚好有网络故障。

同步复制的优势在于追随者始终能够保证和领导者的数据保持一致。当领导者突然宕机，我们仍能够保证追随者副本的数据依旧是有效的。缺点是当同步复制的追随者无响应时（可能发生崩溃，网络错误，或其它的一些原因），写进程将无法继续执行。领导者必须阻塞所有的写操作，直到同步复制重新恢复。

基于这个原因，那么把所有的追随者都设置为同步复制就会显得很不切实际了：因为任何一个节点的中断，都会导致整个系统逐渐停止。实际上，如果允许数据库有同步复制，通常是让其中一个追随者采用同步复制，剩余的追随者则采用异步复制。如果采用同步复制的追随者不可用或复制变慢，那么从异步复制的追随者中挑选一个变为同步复制。这样我们就能够保证至少有两个副本保存有最新的数据：领导者副本和采用同步复制的追随者副本。这种配置机制，我们有时称它为「**半同步-*semi-synchronous***」[[7](#ch5References7)]。

通常，基于领导者复制会被配置称完全同步的。在这种场景下，如果领导者副本发生宕机并且无法恢复时，所有未复制完成的从节点将会丢失这部分写操作数据。这就意味着写入操作不保证能成功的持久化，即使客户端收到了服务端的确认响应。然而，一个完全异步化的配置也有其优势，那就是即使所有的从节点数据同步都落后与领导者副本，也依然不会影响领导者副本的写入。

像「异步复制」这种弱持久性的设计咋看是一个糟糕的折衷办法，但它仍被广泛的应用，尤其是在从节点分布在不同的地理位置这种场景中。我们在[延迟同步问题](#延迟同步问题)这一小节再回过头来讨论这个问题。

---

<center><font face="宋体" size="4" color=black>副本机制探究</font></center>

<font size=2>对于异步复制系统来说，因为领导者副本出错而导致的数据丢失是一个很严重的问题，所以研究人员依然在寻找一种复制方法，既能保证数据不丢失，又能保持很好的性能和可用性。例如，「链式复制-*chain replication* 」[[8](#ch5References8),[9](#ch5References9)] 这种同步复制的变体，就已经在成功的在微软Azure Storage系统中加以应用。</font>

<font size=2>多副本一致性与共识之间存在很强的联系（多个副本之间值达成一致性），我们将在[第九章](chapter9.md)详细讨论这一点。本章我们主要研究的重点是数据库事件中比较简单通用的副本机制。</font>

---

### 配置新的追随者

当你需要增加副本的数量或者替换异常的节点时，就需要考虑增加新的从节点。那我们又如何来保证新加入的从节点数据和主节点的数据保持一致的呢？

如果还是单纯的把数据从一个节点复制到另一个节点通常是不够的，因为客户端会不断的往数据库中写入数据，这就导致数据始终处于不断变化之中，因此常规的文件拷贝方式将会导致不同的节点上呈现出不同时间点的数据，这种情况往往不是我们愿意看到的。

当然你可以锁定数据库（使其不可写）来使得磁盘上的文件保持一致，但使这会违反「高可用」的设计原则。但好在我们可以在不停机的情况下来设置一个从节点。它的执行步骤如下：

1. 在某一时间点对主节点的数据打一个一致性快照。这样可以避免长时间的锁定整个数据库。现在大多数数据库都支持此特性，同样快照也是系统备份所必需的。某些情况下，可能需要借助第三方工具，比如MySQL的*innobackupex*[[12]](#ch5References12)。

2. 复制镜像数据到新的从节点。
3. 从节点连接主节点并请求在同步快照中间所发生的变更数据。这要求快照能够精确的标识出它在复制日志的具体位置。这个位置有多种不同的称呼：如PostgreSQL将其称为「**日志序列号-*log sequence number***」，MySQL称它为「*binlog coordinates*」。
4. 当从节点同步完快照数据后，再处理变更的积压日志的过程，我们称之为追赶。它将继续处理主节点接下来发生的数据变更。

创建从节点的步骤也会因为数据库的不同存在着很大的差异。在某些系统中，这些处理过程完全是自动化的，而在另外一些系统中工作流程又相对会比较复杂，有时甚至需要管理员手动操作。

### 节点宕机处理

系统中的任何一个节点都可能因为故障或计划中系统维护（例如节点重启或安装内核安全补丁）发生宕机，

如果能在不停机的情况下重启节点，将会给运维带来巨大的便利。因此，我们的目标就是在单个节点宕机的情况下，也要保证系统整体可运行，并把节点宕机的影响降至最小。

那么，如何基于领导者复制来保证高可用呢？

#### 从节点失效：追赶式恢复

每个从节点在本地磁盘都维护一份从主节点接收到的变更数据的日志记录。如果从节点崩溃恢复，或者主从节点之间网络暂时中断，从节点也可以轻松的恢复，因为通过本地日志，从节点能够很快查询到在崩溃发生时所处理到的最后一个事务。因此，当从节点重新连上主节点时，再重新请求从崩溃那一刻到恢复期间所有的变更数据就可以了。当把所有的变更数据都同步完成后，便可以像之前一样继续接受来自主节点的数据变更流就可以了。

#### 主节点失效：故障转移

主节点失败处理则会比较棘手：首先要选出一个从节点变更为主节点，客户端要重新配置他们的写请求到新的主节点，其余的从节点也要重新从新的主节点消费变更数据。这个过程便是「**故障转移**」。

故障转移可以切换，主动的通知管理员主节点失效，管理员则会采取一些必要的措施来创建一个新的主节点。也可以自动进行，自动切换的步骤通常如下所示：

1. ***确认主节点失效。***

   导致主节点失效的原因有很多，比如系统崩溃，断电，网络故障等等。目前没有一个万全之策能够精确的检测出到底哪里出现了问题，所以大多数系统都会采用超时机制：节点之间频繁的发送心跳检测信息，如果发现某一节点在较长的一段时间里（例如30s）没有应答，则认为该节点失效（计划中的主节点主动下线维护，不在此考虑范围内）。

2. ***选举新的主节点。***

   可以通过选举的方式在剩余的副本中选出一个作为主节点，或者从之前事先推举的副本中挑选出一个作为主节点。最好的领导者候选人一般是从原有主节点同步数据最多的副本（这样可以最大限度的减少数据丢失）。让所有节点同意某一副本为主节点是典型的共识问题，我们将在[第九章](chapter9.md)详细的讨论。

3. ***重新配置系统配置信息来使新的主节点生效。***

   客户端需要将新的写请求发送给新的主节点（我们在会在[请求路由](chapter6.md#请求路由)这部分讨论）。如果原来的主节点重新上线，它可能并不能感知到其他副本已经强制使其下线而仍然认为自己是主节点。这时候系统要能够确保旧的主节点变为从节点，并能够知晓哪一个是主节点。

   

但是，上述切换过程存在着诸多变数：

* 如果使用异步复制，在旧主节点故障前，新的主节点可能还没有接受到所有来自旧主节点的写入数据。如果等到新的主节点被选举成功后，旧的主节点又重新加入到集群中，那么旧主节点中原有的写入数据将怎么处理呢？同时新的主节点也可能会接收到冲突的写请求数据（旧主节点意识不到自己已经不是主节点，从而还会向其它节点发送同步数据）。针对这种情况，通常的解决方案是原来旧的主节点简单的丢弃掉未复制成功的这部分写数据，但这可能违背客户端的持久化原则。
* 如果有外部的存储系统依赖于数据库的内容做协调数据，那么丢弃写数据将会是特别危险的操作。例如，GitHub的一次事故中[[13](#ch5References13)]，某个未完全同步完成的从节点被晋升为主节点，数据库使用了自增数作为新增行的主键，但是由于新主节点的自增数要落后于原来旧的主节点，所以复用了部分旧主节点已经分配过的数值作为主键。刚好这些主键也被用在了Redis中，造成了MySQL于Redis中的主键不一致问题，从而导致一些私人的数据信息泄漏给了其它的用户。
* 在某些错误的场景下（参见[第八章](chapter8.md)），可能会发生两个节点都认为他们是主节点的情况。这种场景称之为「**脑裂**」，这其实是相当危险的：如果两个主节点都能接收写请求，但是没有处理冲突（参见：[多主复制](#多主复制)），那数据极有可能丢失或损坏。出于安全的应急方案考量，某些系统会制定一些机制，就是当检测到有两个主节点存在时，便会强制关闭其中的一个。当然，如果这种机制设计时考虑的不够周全，也可能会出现两个节点都被关闭的情况[[14](#ch5References14)]。
* 如果设置合适的超时时间来限定主节点的失效状态呢？如果设置的超时时间过长，那么意味着一旦出现主节点失效的情况，恢复所需要的时间也会更长。但是，如果设置的超时时间太短，则会导致不必要的故障转移发生。例如，突发的负载峰值可能会导致节点的响应时间超出所设定的超时时间，或者网络故障也可能会导致数据包的延迟。如果系统已经处在负载峰值或网络故障的情况下，一个不必要的故障转移不会使系统变好，反而变得更糟。

对于这些问题没有很好的解决办法。基于此，一些运维团队宁愿选择手动执行故障切换，也不愿意采用自动故障转移的方案。



















![](../img/figure5-3.png)

<a id="figure5-3"><font color="#A7535A">**图5-3.**</font></a> **用户写之后紧接着伴随着一个读操作，为了防止这种异常现象，我们需要写后读一致性。**

在这种场景下，我们就需要「**写后读一致性-*read-after-write consistency***」，也被称作「**读写一致性-*read-your-writes consistency***」[[24](#ch5References4)]。这样就能够保证用户每次刷新页面，都能够看到他们最新提交的内容。不保证其它用户操作的一致性：其它用户的更新操作可能会到晚些时候才能看到。但是，它保证了用户自己的输入一定能够正确的保存。



### 处理节点异常

### 同步日志实现



## <a id="ReplicationLag">延迟同步问题</a>

### 读己写

### 单调读

### 一致性前缀读

### 同步延时解决方案



## 多主复制

### 多主复制用例

### 写冲突处理

### 多领导者副本拓扑



## 无主复制

### 写时节点故障

### 限定法定数一致性

### Sloppy Quorums and Hinted Handoff



## <a id="detectiongConcurrentWrites">并发写入检测</a>
